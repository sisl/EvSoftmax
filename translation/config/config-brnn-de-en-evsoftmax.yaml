# wmt14_en_de.yaml
save_data: data/iwslt14.tokenized.de-en/run/example
tensorboard: true
tensorboard_log_dir: runs/onmt/de_en

# Corpus opts:
data:
    train:
        path_src: data/iwslt14.tokenized.de-en/train.de
        path_tgt: data/iwslt14.tokenized.de-en/train.en
        transforms: [filtertoolong]
        weight: 1
    valid:
        path_src: data/iwslt14.tokenized.de-en/valid.de
        path_tgt: data/iwslt14.tokenized.de-en/valid.en
        transforms: []

### Transform related opts:
#### Subword
# src_subword_model: /scratch/$USER/checkpoints/en_de/sentencepiece.model
# tgt_subword_model: /scratch/$USER/checkpoints/en_de/sentencepiece.model
# src_subword_type: sentencepiece
# tgt_subword_type: sentencepiece
# onmttok_kwargs: "{'mode': 'none', 'spacer_annotate': True}"

# subword_nbest: 1
# subword_alpha: 0.0
#### Filter
# src_seq_length: 150
# tgt_seq_length: 150

# silently ignore empty lines in the data
skip_empty_level: silent


# # Vocab opts
# ### vocab:
src_vocab: data/iwslt14.tokenized.de-en/vocab.src
tgt_vocab: data/iwslt14.tokenized.de-en/vocab.tgt
src_vocab_size: 32000
tgt_vocab_size: 32000
vocab_size_multiple: 8
src_words_min_frequency: 1
tgt_words_min_frequency: 1
share_vocab: True

# # Model training parameters

# General opts
save_model: /scratch/$USER/checkpoints/de_en/evsoftmax_lr_0p1
train_from: /scratch/$USER/checkpoints/de_en/iwslt-brnn2.s131_acc_62.71_ppl_7.74_e20.pt
keep_checkpoint: 10
save_checkpoint_steps: 5000
# average_decay: 0.0005
seed: 1234
report_every: 100
train_steps: 50000
valid_steps: 1000

# Batching
queue_size: 256
bucket_size: 8192
pool_factor: 2048
world_size: 1
gpu_ranks: [0]
batch_type: "sents"
batch_size: 64
valid_batch_size: 32
batch_size_multiple: 1
max_generator_batches: 32
accum_count: 1
accum_steps: [0]

# Optimization
model_dtype: "fp32"
optim: "sgd"
learning_rate: 0.1
learning_rate_decay: 0.5
start_decay_at: 8
warmup_steps: 4000
decay_method: "noam"
adam_beta1: 0.9
adam_beta2: 0.999
max_grad_norm: 5
label_smoothing: 0.0
param_init: 0.1
param_init_glorot: true
normalization: "sents"

# Model
src_word_vec_size: 500
tgt_word_vec_size: 500
word_vec_size: -1
share_decoder_embeddings: False
share_embeddings: False
position_encoding: False
feat_merge: 'concat'
feat_vec_size: -1
feat_vec_exponent: 0.7
model_type: 'text'
encoder_type: 'brnn'
decoder_type: 'rnn'
layers: -1
enc_layers: 2
dec_layers: 2
rnn_size: 500
cnn_kernel_width: 3
input_feed: 1
rnn_type: 'LSTM'
brnn_merge: 'concat'
global_attention: 'general'
copy_attn: False
copy_attn_force: False
reuse_copy_attn: False
copy_loss_by_seqlength: False
coverage_attn: True
lambda_coverage: 1
pre_word_vecs_enc: None
pre_word_vecs_dec: None
fix_word_vecs_enc: False
fix_word_vecs_dec: False
adagrad_accumulator_init: 0
dropout: 0.3
truncated_decoder: 0
sample_rate: 16000
window_size: 0.02

self_attn_function: evsoftmax
generator_function: evsoftmax
